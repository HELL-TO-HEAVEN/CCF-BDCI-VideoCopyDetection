{"cells":[{"cell_type":"markdown","metadata":{},"source":["## 特征提取"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"import os, sys, codecs\nimport glob\nimport pandas as pd\nimport numpy as np\nimport pickle\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport cv2\n\nfrom sklearn.preprocessing import normalize as sknormalize\nfrom sklearn.decomposition import PCA\n\nimport torch\ntorch.manual_seed(0)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data.dataset import Dataset\n\nimport logging\nlogging.basicConfig(level = logging.DEBUG, filename = 'example.log',\n                    format = '%(asctime)s - %(filename)s[line:%(lineno)d]: %(message)s')  # \n\nPATH = '/home/wx/work/video_copy_detection/'\nTRAIN_PATH = PATH + 'train/'\nTEST_PATH = PATH + 'test/'\nTRAIN_QUERY_PATH = TRAIN_PATH + 'query/'\nREFER_PATH = TRAIN_PATH + 'refer/'\nTRAIN_QUERY_FRAME_PATH = TRAIN_PATH + 'query_frame/'\nREFER_FRAME_PATH = TRAIN_PATH + 'refer_frame/'\nTEST_QUERY_PATH = TEST_PATH + 'query/'\nTEST_QUERY_FRAME_PATH = TEST_PATH + 'query_frame/'\nCODE_DIR = PATH + 'code/'"},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":"class QRDataset(Dataset):\n    def __init__(self, img_path, transform = None):\n        self.img_path = img_path\n\n        self.img_label = np.zeros(len(img_path))\n    \n        if transform is not None:\n            self.transform = transform\n        else:\n            self.transform = None\n    \n    def __getitem__(self, index):\n        img = Image.open(self.img_path[index])\n        \n        if self.transform is not None:\n            img = self.transform(img)\n        \n        return img, self.img_path[index]\n\n    def __len__(self):\n        return len(self.img_path)\n\nclass Img2Vec():\n\n    def __init__(self, model='resnet-18', layer='default', layer_output_size=512):\n        \"\"\" Img2Vec\n        :param model: String name of requested model\n        :param layer: String or Int depending on model.\n        :param layer_output_size: Int depicting the output size of the requested layer\n        \"\"\"\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.layer_output_size = layer_output_size\n        self.model_name = model\n        \n        self.model, self.extraction_layer = self._get_model_and_layer(model, layer)\n\n        self.model = self.model.to(self.device)\n\n        self.model.eval()\n\n        self.transformer = transforms.Compose([\n            transforms.Resize((224, 224)), \n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n            ])\n\n    def get_vec(self, path):\n        \"\"\" Get vector embedding from PIL image\n        :param path: Path of image dataset\n        :returns: Numpy ndarray\n        \"\"\"\n        if not isinstance(path, list):\n            path = [path]\n\n        data_loader = torch.utils.data.DataLoader(QRDataset(path, self.transformer), batch_size = 40, \n                                                  shuffle = False, num_workers = 16)\n\n        my_embedding = []\n\n        # hook function\n        def append_data(module, input, output):\n            my_embedding.append(output.clone().detach().cpu().numpy())\n    \n        with torch.no_grad():\n            for batch_data in tqdm(data_loader):\n                batch_x, batch_y = batch_data\n                if torch.cuda.is_available():\n                    batch_x = Variable(batch_x, requires_grad = False).cuda()\n                else:\n                    batch_x = Variable(batch_x, requires_grad = False)\n\n                h = self.extraction_layer.register_forward_hook(append_data)\n                h_x = self.model(batch_x)\n                h.remove()\n                del h_x\n\n        my_embedding = np.vstack(my_embedding)\n        if self.model_name == 'alexnet':\n            return my_embedding[:, :]\n        else:\n            return my_embedding[:, :, 0, 0]\n\n    def _get_model_and_layer(self, model_name, layer):\n        \"\"\" Internal method for getting layer from model\n        :param model_name: model name such as 'resnet-18'\n        :param layer: layer as a string for resnet-18 or int for alexnet\n        :returns: pytorch model, selected layer\n        \"\"\"\n        if model_name == 'resnet-18':\n            model = models.resnet18(pretrained=True)\n            if layer == 'default':\n                layer = model._modules.get('avgpool')\n                self.layer_output_size = 512\n            else:\n                layer = model._modules.get(layer)\n\n            return model, layer\n\n        elif model_name == 'alexnet':\n            model = models.alexnet(pretrained=True)\n            if layer == 'default':\n                layer = model.classifier[-2]\n                self.layer_output_size = 4096\n            else:\n                layer = model.classifier[-layer]\n\n            return model, layer\n\n        else:\n            raise KeyError('Model %s was not found' % model_name)"},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":"# 读取 test_query 视频的关键帧，并按照视频和关键帧时间进行排序\ntest_query_imgs_path = []\nfor id in pd.read_csv(TEST_PATH + 'submit_example.csv')['query_id']:\n    test_query_imgs_path += glob.glob(TEST_QUERY_FRAME_PATH + id + '/*.jpg')\n\ntest_query_imgs_path.sort(key = lambda x: x.lower())"},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":"# 读取 train_query 视频的关键帧，并按照视频和关键帧时间进行排序\ntrain_query_imgs_path = []\nfor id in pd.read_csv(TRAIN_PATH + 'train.csv')['query_id']:\n    train_query_imgs_path += glob.glob(TRAIN_QUERY_FRAME_PATH + id + '/*.jpg')\n\ntrain_query_imgs_path.sort(key = lambda x: x.lower())"},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":"# 读取 refer 视频的关键帧，并按照视频和关键帧时间进行排序\n\nrefer_imgs_path = glob.glob(REFER_FRAME_PATH + '*/*.jpg')\nrefer_imgs_path.sort(key = lambda x: x.lower())"},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":"# Initialize Img2Vec\nimg2vec = Img2Vec()"},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 1564/1564 [01:00<00:00, 25.92it/s]\n"}],"source":"# 抽取 test_query 关键帧特征\ntest_query_features = img2vec.get_vec(test_query_imgs_path[:])"},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 3128/3128 [02:01<00:00, 25.77it/s]\n"}],"source":"# 抽取 train_query 关键帧特征\ntrain_query_features = img2vec.get_vec(train_query_imgs_path[:])"},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 4527/4527 [03:04<00:00, 24.60it/s]\n"}],"source":"# 抽取 refer 关键帧特征\nrefer_features = img2vec.get_vec(list(refer_imgs_path[:]))"},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":"def normalize(x, copy = False):\n    \"\"\"\n    A helper function that wraps the function of the same name in sklearn.\n    This helper handles the case of a single column vector.\n    \"\"\"\n    if type(x) == np.ndarray and len(x.shape) == 1:\n        return np.squeeze(sknormalize(x.reshape(1, -1), copy = copy))\n        #return np.squeeze(x / np.sqrt((x ** 2).sum(-1))[..., np.newaxis])\n    else:\n        return sknormalize(x, copy = copy)\n        #return x / np.sqrt((x ** 2).sum(-1))[..., np.newaxis]"},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"text/plain":"'\\npca = PCA(n_components=512)\\n\\ntrain_query_features = pca.fit_transform(train_query_features)\\ntest_query_features = pca.fit_transform(test_query_features)\\nrefer_features = pca.fit_transform(refer_features)\\n'"},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":"# PCA 降维\n'''\npca = PCA(n_components=512)\n\ntrain_query_features = pca.fit_transform(train_query_features)\ntest_query_features = pca.fit_transform(test_query_features)\nrefer_features = pca.fit_transform(refer_features)\n'''"},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":"# L2正则化\ntrain_query_features = normalize(train_query_features)\ntest_query_features = normalize(test_query_features)\nrefer_features = normalize(refer_features)"},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":"# 保存 test_query 关键帧特征\n\nwith open(PATH + 'var/test_query_features.pk', 'wb') as pk_file:\n    pickle.dump(test_query_features, pk_file)"},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":"# 保存 train_query 关键帧特征\n\nwith open(PATH + 'var/train_query_features.pk', 'wb') as pk_file:\n    pickle.dump(train_query_features, pk_file)"},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":"# 保存 refer 关键帧特征\n\nwith open(PATH + 'var/refer_features.pk', 'wb') as pk_file:\n    pickle.dump(refer_features, pk_file)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}